{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK6lfDe9LB0q"
      },
      "source": [
        "# Buggy Coders - Whale A call recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgUZJbV97cah"
      },
      "outputs": [],
      "source": [
        "# the libararies imported are as follows\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import aifc\n",
        "from tqdm import tqdm\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import glob\n",
        "from skimage.transform import resize\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib import mlab\n",
        "import pandas as pd\n",
        "import keras\n",
        "from joblib import dump, load\n",
        "from keras.optimizers import SGD, Adam, Adagrad, RMSprop\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Flatten, Conv2D, MaxPooling2D\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score, log_loss\n",
        "from keras.models import load_model, Sequential\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "import warnings\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AluIIWODQrER"
      },
      "outputs": [],
      "source": [
        "# convert all wav files to aiff files using audio segment package\n",
        "\n",
        "AudioSegment.converter = \"/usr/bin/ffmpeg\"  # using ffmpeg package as a dependency for audio segment\n",
        "\n",
        "\n",
        "def convert_to_aiff(file_path, new_file_path):\n",
        "    # Load the .wav file\n",
        "    audio = AudioSegment.from_wav(file_path)\n",
        "    # Save the .aiff file\n",
        "    audio.export(new_file_path, format=\"aiff\")\n",
        "\n",
        "# Converting training files\n",
        "# List the directories containing the .wav files\n",
        "labels = [\"1\", \"0\"]\n",
        "to_directory = \"aiff/train/\"\n",
        "directories = [\"train/train/0\", \"train/train/1\"]\n",
        "\n",
        "# Get a list of all the .wav files in the directory\n",
        "for directory in directories:\n",
        "  label = directory[-1]\n",
        "  wav_files = [f for f in os.listdir(directory) if f.endswith(\".wav\")]\n",
        "  # Loop through each .wav file\n",
        "  for wav_file in tqdm(wav_files, desc=directory):\n",
        "      # Get the file path of the .wav file\n",
        "      file_path = os.path.join(directory, wav_file)\n",
        "      to_file_path = os.path.join(to_directory, wav_file)\n",
        "      # Get the new file path for the .aiff file\n",
        "      new_file_path = os.path.splitext(to_file_path)[0] + label + \".aiff\"\n",
        "      # Convert the .wav file to .aiff format\n",
        "      convert_to_aiff(file_path, new_file_path)\n",
        "\n",
        "\n",
        "# Converting test files\n",
        "to_test_directory = \"aiff/test/\"\n",
        "from_test_directory = \"test/test\"\n",
        "# Get a list of all the .wav files in the from_test_directory\n",
        "wav_files = [f for f in os.listdir(from_test_directory) if f.endswith(\".wav\")]\n",
        "# Loop through each .wav file\n",
        "for wav_file in tqdm(wav_files, desc=from_test_directory):\n",
        "    # Get the file path of the .wav file\n",
        "    file_path = os.path.join(from_test_directory, wav_file)\n",
        "    to_file_path = os.path.join(to_test_directory, wav_file)\n",
        "    # Get the new file path for the .aiff file\n",
        "    new_file_path = os.path.splitext(to_file_path)[0] + \".aiff\"\n",
        "    # Convert the .wav file to .aiff format\n",
        "    convert_to_aiff(file_path, new_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fz3kV4WS8wH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Pre Procesing:\n",
        "\n",
        "We performed a pre-processing step to improve the horizontal (temporal dimension) and vertical (frequency dimension)\n",
        "contrast of the spectrogram images which proved to be efficient in getting better results. Both kinds of contrast enhance-\n",
        "ment were achieved by creating two moving average filters with only difference in their length (denoted filter A and filter B).\n",
        "\n",
        "1.1 Temporal dimension:\n",
        "\n",
        "Each row of the spectrogram was combined once with filter A and then with filter B. Since filter a had a much smaller length\n",
        "compared to filter B the values of the output of the combination with filter A represented local averages for adjacent pixels\n",
        "whereas the output of the combination with filter B represented global averages for neighborhoods of pixels. Contrast\n",
        "enhancement was achieved by subtracting out these local averages from their corresponding rows, thereby emphasizing\n",
        "more significant temporal changes.Contrast enhancement was performed by subtracting out the local averages from their\n",
        "corresponding rows, thereby emphasizing more prominent temporal changes.\n",
        "\n",
        "1.2 Frequency domain:\n",
        "\n",
        "Each column of the spectrogram was combined sperately with filter A and filter B for contrast enhancement and to empha-\n",
        "size more on significant differences in the frequency power distribution for every time step, the output of the combination\n",
        "operation with filter A is subtracted from that of filter B, and the actual local averages subtracted from each corresponding\n",
        "row.\n",
        "\n",
        "Two new kinds of spectrogram images are produced from one original sample, thus doubling the number of samples in the\n",
        "dataset.\n",
        "\"\"\"\n",
        "def ReadAIFF(file):\n",
        "    ''' ReadAIFF Method\n",
        "            Read AIFF and convert to numpy array\n",
        "            \n",
        "            Args: \n",
        "                file: string file to read \n",
        "            Returns:\n",
        "                numpy array containing whale audio clip      \n",
        "                \n",
        "    '''\n",
        "    s = aifc.open(file,'r')\n",
        "    nFrames = s.getnframes()\n",
        "    strSig = s.readframes(nFrames)\n",
        "    return np.frombuffer(strSig,np.short).byteswap()\n",
        "\n",
        "def SpecGram(file,params=None):\n",
        "    '''  The  function takes an audio file (specified as a string file), and creates a spectrogram representation of it.\n",
        "         It  pre-process input for input shape uniformity \n",
        "         Args: 1. string file to read \n",
        "               2. The parameters for creating the spectrogram are passed as a dictionary in the \"params\" argument.  \n",
        "         The output of the function is a pre-processed spectrogram matrix and arrays for the frequency and time bins, which are one-dimensional.\n",
        "                \n",
        "    '''\n",
        "    s = ReadAIFF(file)\n",
        "    # Convert to spectrogram \n",
        "    P,freqs,bins = mlab.specgram(s,**params)\n",
        "    m,n = P.shape\n",
        "    # Ensure all image inputs to the CNN are the same size. If the number of time bins \n",
        "    # is less than 59, pad with zeros \n",
        "    if n < 59:\n",
        "        Q = np.zeros((m,59))\n",
        "        Q[:,:n] = P\n",
        "    else:\n",
        "        Q = P\n",
        "    return Q,freqs,bins\n",
        "\n",
        "def slidingWindowV(P,inner=3,outer=64,maxM=50,minM=7,maxT=59,norm=True):\n",
        "    ''' Enhance the contrast along frequency dimension '''\n",
        "\n",
        "    Q = P.copy()\n",
        "    m, n = Q.shape\n",
        "    if norm:\n",
        "        #segment to remove  extreme values \n",
        "        mval, sval = np.mean(Q[minM:maxM,:maxT]), np.std(Q[minM:maxM,:maxT])\n",
        "        fact_ = 1.5\n",
        "        Q[Q > mval + fact_*sval] = mval + fact_*sval\n",
        "        Q[Q < mval - fact_*sval] = mval - fact_*sval\n",
        "        Q[:minM,:] = mval\n",
        "    # Setting up the local mean window \n",
        "    wInner = np.ones(inner)\n",
        "    # Setting up the overall mean window \n",
        "    wOuter = np.ones(outer)\n",
        "    # Removing  overall mean and local mean using np.convolve \n",
        "   \n",
        "    for i in range(maxT):\n",
        "        Q[:,i] = Q[:,i] - (np.convolve(Q[:,i],wOuter,'same') - np.convolve(Q[:,i],wInner,'same'))/(outer - inner)\n",
        "    Q[Q < 0] = 0.\n",
        "    return Q[:maxM,:]\n",
        "\n",
        "def slidingWindowH(P,inner=3,outer=32,maxM=50,minM=7,maxT=59,norm=True):\n",
        "    ''' Enhance the contrast along temporal dimension '''\n",
        "    Q = P.copy()\n",
        "    m, n = Q.shape\n",
        "    if outer > maxT:\n",
        "        outer = maxT\n",
        "    if norm:\n",
        "        # Cutting off extreme values \n",
        "        mval, sval = np.mean(Q[minM:maxM,:maxT]), np.std(Q[minM:maxM,:maxT])\n",
        "        fact_ = 1.5\n",
        "        Q[Q > mval + fact_*sval] = mval + fact_*sval\n",
        "        Q[Q < mval - fact_*sval] = mval - fact_*sval\n",
        "        Q[:minM,:] = mval\n",
        "    # Setting up the local mean window and overall mean window \n",
        "    wInner = np.ones(inner)\n",
        "    wOuter = np.ones(outer)\n",
        "    if inner > maxT:\n",
        "        return Q[:maxM,:]\n",
        "    # Removing overall mean and local mean using np.convolve     \n",
        "    \n",
        "    for i in range(maxM):\n",
        "        Q[i,:maxT] = Q[i,:maxT] - (np.convolve(Q[i,:maxT],wOuter,'same') - np.convolve(Q[i,:maxT],wInner,'same'))/(outer - inner)\n",
        "    Q[Q < 0] = 0.\n",
        "    return Q[:maxM,:]\n",
        "\n",
        "\n",
        "def get_file_id(file):\n",
        "    id,extension = os.path.splitext(file)\n",
        "    return id\n",
        "\n",
        "def extract_labels(file):\n",
        "    name,extension = os.path.splitext(file)\n",
        "    label = name[-1]\n",
        "    return int(label)\n",
        "\n",
        "def extract_featuresV(file,params=None):\n",
        "    '''The function is used for obtaining a spectrogram representation of an audio file with vertically-enhanced contrast.\n",
        "       suitable for input into a Convolutional Neural Network (CNN).\n",
        "       The audio file is specified as a string in the \"file\" argument,\n",
        "       and the spectrogram parameters are passed in as a dictionary in the \"params\" argument. \n",
        "       The output of the function is a 2-dimensional numpy array, which is an image with vertically-enhanced contrast.    \n",
        "                \n",
        "    '''\n",
        "    P,freqs,bins = SpecGram(file,params)\n",
        "    Q = slidingWindowV(P,inner=3,maxM=50,maxT=bins.size)\n",
        "    # Resize spectrogram image into a square matrix \n",
        "    Q = resize(Q,(64,64),mode='edge')\n",
        "    return Q\n",
        "\n",
        "def extract_featuresH(file,params=None):\n",
        "    ''' The function is used  for obtaining a spectrogram of an audio file with an emphasis on horizontal contrast, \n",
        "       suitable for input into a Convolutional Neural Network (CNN). \n",
        "       The audio file is specified as a string in the \"file\" argument, and \n",
        "       the parameters for creating the spectrogram are passed in as a dictionary in the \"params\" argument. \n",
        "       The result of the function is a 2-dimensional numpy array, which is an image with horizontally-enhanced contrast. \n",
        "                 \n",
        "                \n",
        "    '''\n",
        "    P,freqs,bins = SpecGram(file,params)\n",
        "    W = slidingWindowH(P,inner=3,outer=32,maxM=50,maxT=bins.size)\n",
        "    # Resize spectrogram image into a square matrix \n",
        "    W= resize(W,(64,64),mode='edge')\n",
        "    return W\n",
        "\n",
        "\n",
        "def get_training_data():\n",
        "    ''' method of obtaining data'''\n",
        "\n",
        "    # Spectrogram parameters \n",
        "    params = {'NFFT':256,'Fs':2000,'noverlap':192}\n",
        "    # Load in the audio files from the training dataset\n",
        "    path = 'aiff/train'\n",
        "    filenames = glob.glob(path+'/*.aiff')\n",
        "    \"\"\"\n",
        "     For each audio file, we obation  the spectrograms with vertically-enhanced contrast \n",
        "     and the spectrograms with horizontally-enhanced contrast. This in \n",
        "     effect doubles the amount of data for training, and presents the CNN with different \n",
        "     perspectives of the same spectrogram image of the original audio file \n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print('Extracting Training Features')\n",
        "    training_featuresV = np.array([extract_featuresV(x,params=params) for x in tqdm(filenames, desc=\"Extracting test features V\")])\n",
        "    training_featuresH = np.array([extract_featuresH(x,params=params) for x in tqdm(filenames, desc=\"Extracting test features H\")])\n",
        "    # Concatenate the two feature matrices together to form a double-length feature matrix\n",
        "    X_train = np.append(training_featuresV,training_featuresH,axis=0)\n",
        "    # Axis 0 indicates the number of examples, Axis 1 and 2 are the features (64x64 image\n",
        "    # spectrograms). Add Axis 3 to indicate 1 channel (depth of 1 for spectrogram image) for \n",
        "    # compatibility with Keras CNN model \n",
        "    X_train = X_train[:,:,:,np.newaxis]\n",
        "    \n",
        "    \n",
        "    # Extract labels for the training dataset. Since the vertically-enhanced and \n",
        "    # horizontally-enhanced images are concatenated to form a training dataset twice as long,\n",
        "    # append a duplicate copy of the training labels to form a training label vector \n",
        "    # twice as long \n",
        "\n",
        "    print('Extracting Training Labels')\n",
        "    Y_train = np.array([extract_labels(x) for x in tqdm(filenames, desc=\"Extracting training labels\")])\n",
        "    Y_train = np.append(Y_train,Y_train)\n",
        "\n",
        "\n",
        "    # Do not append a duplicate copy of the test labels to form a test label vector twice\n",
        "    # as long, since the two feature matrices were not concatenated together previously. \n",
        "    # The number of elements in the test label vector is the number of original audio\n",
        "    # files in the test dataset \n",
        "\n",
        "    return X_train, Y_train\n",
        "\n",
        "def get_test_data():\n",
        "  \n",
        "    # Spectrogram parameters \n",
        "    params = {'NFFT':256,'Fs':2000,'noverlap':192}\n",
        "    # Load in the audio files from the test dataset\n",
        "    path = 'aiff/test'\n",
        "    # filenames = glob.glob(path+'/[10]/*.aiff')\n",
        "    filenames = glob.glob(path+'/*.aiff')\n",
        "\n",
        "    # For each audio file, extract the spectrograms with vertically-enhanced contrast \n",
        "    # separately from the spectrograms with horizontally-enhanced contrast. This in \n",
        "    # effect doubles the amount of data for training, and presents the CNN with different \n",
        "    # perspectives of the same spectrogram image of the original audio file \n",
        "\n",
        "\n",
        "    print('Extracting File id')\n",
        "    file_id = np.array([get_file_id(x) for x in tqdm(filenames, desc=\"Extracting File id\")])\n",
        "    file_id = np.append(file_id,file_id)\n",
        "\n",
        "\n",
        "    print('Extracting Test Features')\n",
        "    X_testV = np.array([extract_featuresV(x,params=params) for x in tqdm(filenames, desc=\"Extracting test features V\")])\n",
        "    X_testH = np.array([extract_featuresH(x,params=params) for x in tqdm(filenames, desc=\"Extracting test features H\")])\n",
        "    X_testV = X_testV[:,:,:,np.newaxis]\n",
        "    X_testH = X_testH[:,:,:,np.newaxis]\n",
        "    X_test = np.append(X_testV,X_testV,axis=0)\n",
        "    X_test = X_test[:,:,:,np.newaxis]\n",
        "\n",
        "\n",
        "    return file_id, X_test, X_testV, X_testH\n",
        "\n",
        "\n",
        "X_train, Y_train = get_training_data()\n",
        "file_id, X_test, X_testV, X_testH = get_test_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgsg1ozZGl6Y"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A spectrogram was obtained from every audio file in the training set The spectrogram’s contrast was enhanced hori-\n",
        "zontally (temporal dimension) and vertically (frequency dimension) by removing extreme values and implementing\n",
        "a sliding mean.\n",
        "• The CNN model’s hyperparameters were optimized using 3-Fold Cross Validation via GridSearchCV. The CNN\n",
        "was fit using the enhanced training set\n",
        "• .If the vertically-enhanced input yielded 1 and the horizontally-enhanced input yielded 0, the final predicted label\n",
        "was 1.\n",
        "• We choosed The Receiver Operating Characteristic (ROC) for evaluation, being a measure of the true positive rate vs\n",
        "false positive rate as the discrimination threshold of the binary classifier is varied. The area under the curve (AUC)\n",
        "is a single number metric of a binary classifier’s ROC curve and it is this ROC-AUC score that is used for evaluation\n",
        "of the CNN model.\n",
        "\"\"\"\n",
        "class CNNModel(object):\n",
        "    def __init__(self, learning_rate=0.01, activation='relu', optimizer=SGD):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.activation = activation\n",
        "        self.optimizer = optimizer\n",
        "        self.classes_ = [0, 1]\n",
        "\n",
        "    def create_model(self):\n",
        "        model = Sequential()\n",
        "        # Dropout on the visible layer (1 in 5 probability of dropout) \n",
        "        model.add(Dropout(0.2,input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3]),name='drop1'))\n",
        "        # Conv2D -> BatchNorm -> Relu Activation -> MaxPooling2D\n",
        "        model.add(Conv2D(15,(7,7),strides=(1,1),name='conv1'))\n",
        "        model.add(BatchNormalization(axis=3,name='bn1'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D((2,2),name='max_pool1'))\n",
        "        # Conv2D -> BatchNorm -> Relu Activation -> MaxPooling2D\n",
        "        model.add(Conv2D(30,(7,7),strides=(1,1),name='conv2'))\n",
        "        model.add(BatchNormalization(axis=3,name='bn2'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D((2,2),name='max_pool2'))\n",
        "        # Flatten to yield input for the fully connected layer \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(200,activation='relu',name='fc1'))\n",
        "        # Dropout on the fully connected layer (1 in 2 probability of dropout) \n",
        "        model.add(Dropout(0.5,name='drop2'))\n",
        "        # Single unit output layer with sigmoid nonlinearity \n",
        "        model.add(Dense(1,activation='sigmoid',name='fc2'))\n",
        "        # Use Stochastic Gradient Discent for optimization \n",
        "        sgd = SGD(learning_rate=0.1,decay=0.005,nesterov=False)\n",
        "        model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def fit(self, X, y, batch_size=128, epochs=10):\n",
        "        self.model = self.create_model()\n",
        "        self.model.fit(X, y, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        y_pred = self.model.predict(X)\n",
        "        return np.column_stack((1 - y_pred, y_pred))\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.round(self.model.predict(X))\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {'learning_rate': self.learning_rate, 'activation': self.activation}\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        self.learning_rate = params['learning_rate']\n",
        "        self.activation = params['activation']\n",
        "        self.optimizer = params['optimizer']\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leogjUCWGwBq"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameters to search\n",
        "param_dist = {'learning_rate': [0.1, 0.01],\n",
        "            'batch_size': [128, 2*128, 3*128],\n",
        "            'epochs': [20, 30, 40],\n",
        "            'optimizer': [SGD, Adam, Adagrad, RMSprop],\n",
        "            'activation': ['tanh', 'relu', 'sigmoid']}\n",
        "\n",
        "# Create the scoring function using the f1_score\n",
        "f1_scorer = make_scorer(f1_score, average='micro')\n",
        "\n",
        "\n",
        "# Create the GridSearch object\n",
        "grid_search = GridSearchCV(CNNModel(), param_grid=param_dist, scoring=f1_scorer, cv=2, verbose=3, return_train_score=True)\n",
        "# Fit the randomized search object to the training data\n",
        "print('Fitting model...')\n",
        "grid_result = grid_search.fit(X_train, Y_train)\n",
        "\n",
        "print('Best: %f using %s' % (grid_result.best_score_,grid_result.best_params_))\n",
        "# Print mean and standard deviation of accuracy scores for each combination\n",
        "# of parameters evaluated by GridSearchCV\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean,std,param in zip(means,stds,params):\n",
        "    print('%f (%f) with: %r' % (mean,std,param))\n",
        "\n",
        "# save the best model\n",
        "print('Saving the best model...')\n",
        "best_model = grid_result.best_estimator_\n",
        "dump(best_model, 'best_model.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Shpy363YG-QH"
      },
      "outputs": [],
      "source": [
        "print(\"Generating predictions...\")\n",
        "Y_predV = best_model.predict(X_testV)\n",
        "Y_predH = best_model.predict(X_testH)\n",
        "Y_pred = Y_predV + Y_predH\n",
        "Y_pred[Y_pred>1] = 1\n",
        "\n",
        "# reshape Y_pred to be a list of integers\n",
        "Y_pred = Y_pred.reshape(Y_pred.shape[0])\n",
        "Y_pred = [int(round(y)) for y in Y_pred]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-ioQJhdHCGb"
      },
      "outputs": [],
      "source": [
        "# create submission file\n",
        "print(\"Creating submission file...\")\n",
        "submission = pd.DataFrame({'id':file_id,'label':Y_pred})\n",
        "submission.to_csv('submission.csv',index=False)\n",
        "print(\"Done\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
